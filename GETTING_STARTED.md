"""
PROJECT STRUCTURE AND GETTING STARTED GUIDE

This file provides a comprehensive overview of the ingestion service structure
and quick-start instructions.
"""

PROJECT_STRUCTURE = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         BINANCE REAL-TIME INGESTION SERVICE - PROJECT STRUCTURE              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ingestion/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                              # Main documentation
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md                        # Detailed architecture document
â”œâ”€â”€ ğŸ“„ requirements.txt                       # Python dependencies
â”œâ”€â”€ ğŸ“„ .env                                   # Environment variables (local)
â”œâ”€â”€ ğŸ“„ .env.example                           # Environment template
â”œâ”€â”€ ğŸ“„ Dockerfile                             # Docker container definition
â”œâ”€â”€ ğŸ“„ docker-compose.yml                     # Docker Compose for full stack
â”‚
â”œâ”€â”€ ğŸ”§ main.py                                # Service entry point
â”œâ”€â”€ ğŸ”§ setup_kafka.py                         # Kafka topic initialization
â”œâ”€â”€ ğŸ”§ quickstart.py                          # Quick start guide
â”œâ”€â”€ ğŸ”§ monitoring.py                          # Metrics and monitoring utilities
â”œâ”€â”€ ğŸ”§ consumer_example.py                    # Example Kafka consumer
â”‚
â”œâ”€â”€ ğŸ“ config/                                # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py                           # Pydantic settings (BinanceConfig, KafkaConfig, etc.)
â”‚
â”œâ”€â”€ ğŸ“ src/                                   # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py                             # Structured logging setup
â”‚   â”œâ”€â”€ binance_client.py                     # Main WebSocket client (CORE)
â”‚   â””â”€â”€ kafka_producer.py                     # Kafka producer wrapper (CORE)
â”‚
â””â”€â”€ ğŸ“ tests/                                 # Unit tests
    â”œâ”€â”€ conftest.py                           # Pytest configuration
    â””â”€â”€ test_ingestion.py                     # Test suite


CORE COMPONENTS EXPLAINED:

1ï¸âƒ£  binance_client.py (444 lines)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Main component: BinanceWebSocketClient
    
    Features:
    â€¢ Multi-stream WebSocket subscription to Binance
    â€¢ Automatic reconnection with exponential backoff
    â€¢ Event enrichment (timestamps, metadata, event type mapping)
    â€¢ Message batching (configurable size and timeout)
    â€¢ Thread-safe in-memory buffering
    â€¢ Graceful shutdown on signals
    â€¢ Comprehensive metrics collection
    
    Key Methods:
    â€¢ connect()                    - Establish WebSocket connection
    â€¢ run()                        - Main async loop with auto-reconnect
    â€¢ start_in_background()        - Run in background thread
    â€¢ stop()                       - Graceful shutdown
    â€¢ get_metrics()                - Retrieve performance metrics
    
    Configuration:
    â€¢ BINANCE_SYMBOLS              - Trading pairs to monitor
    â€¢ BINANCE_STREAM_TYPES         - Event types (aggTrade, depth, etc)
    â€¢ RECONNECT_INTERVAL           - Base reconnect delay (5s default)
    â€¢ MAX_RETRIES                  - Max reconnection attempts (5 default)
    â€¢ BATCH_SIZE                   - Messages per batch (100 default)
    â€¢ BATCH_TIMEOUT_SECONDS        - Batch flush timeout (5s default)


2ï¸âƒ£  kafka_producer.py (233 lines)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Main component: MarketEventProducer
    
    Features:
    â€¢ JSON serialization of market events
    â€¢ Snappy compression for efficient transport
    â€¢ Async callbacks for success/error handling
    â€¢ Automatic retry (3 attempts)
    â€¢ Topic routing by event type
    â€¢ Partition key selection (Symbol for ordering)
    â€¢ Producer metrics tracking
    
    Key Methods:
    â€¢ send_event()                 - Send single event to Kafka
    â€¢ send_batch()                 - Send batch of events
    â€¢ flush()                      - Force pending message flush
    â€¢ close()                      - Graceful shutdown
    â€¢ get_metrics()                - Retrieve producer metrics
    
    Kafka Configuration:
    â€¢ KAFKA_BOOTSTRAP_SERVERS      - Broker addresses
    â€¢ KAFKA_TOPIC_RAW_EVENTS       - Raw events topic
    â€¢ KAFKA_TOPIC_TRADES           - Trade events topic
    â€¢ KAFKA_TOPIC_DEPTH            - Depth updates topic
    â€¢ KAFKA_PARTITIONS             - Number of partitions (3 default)
    â€¢ KAFKA_COMPRESSION            - Compression type (snappy default)


3ï¸âƒ£  config/settings.py (101 lines)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Configuration Management using Pydantic
    
    Classes:
    â€¢ BinanceConfig               - Binance API settings
    â€¢ KafkaConfig                 - Kafka broker settings
    â€¢ ConnectionConfig            - Retry and batching settings
    â€¢ LoggingConfig               - Logging configuration
    â€¢ Settings                    - Master settings class
    
    Features:
    â€¢ Type validation
    â€¢ Environment variable loading
    â€¢ Default values
    â€¢ Centralized configuration


4ï¸âƒ£  src/logger.py (65 lines)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Structured Logging Setup
    
    Features:
    â€¢ JSON-formatted logs
    â€¢ Context-aware logging
    â€¢ Exception capture with stack traces
    â€¢ Integration with structlog
    
    Functions:
    â€¢ setup_logging()              - Initialize logging system
    â€¢ get_logger()                 - Get logger instance
    â€¢ LogContext                   - Context manager for operation logging


================================================================================
QUICK START GUIDE
================================================================================

OPTION 1: Docker Compose (Recommended for Development)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Start all services:
   $ docker-compose up -d
   
   This starts:
   â€¢ Zookeeper (metadata storage)
   â€¢ Kafka broker (message queue)
   â€¢ Kafka UI (web interface at http://localhost:8080)
   â€¢ Binance ingestion service

2. View logs:
   $ docker-compose logs -f binance-ingestion

3. Access Kafka UI:
   Open http://localhost:8080 in your browser
   
4. Inspect topics:
   $ docker exec ingestion_kafka_1 kafka-topics --list --bootstrap-server localhost:29092

5. View messages:
   $ docker exec ingestion_kafka_1 kafka-console-consumer \\
       --bootstrap-server localhost:29092 \\
       --topic binance-trades \\
       --from-beginning \\
       --max-messages 5

6. Stop services:
   $ docker-compose down


OPTION 2: Local Development with Python
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Prerequisites:
â€¢ Python 3.11+
â€¢ Kafka and Zookeeper running (docker-compose up -d kafka zookeeper)

1. Create virtual environment:
   $ python -m venv venv
   $ source venv/bin/activate          # macOS/Linux
   # OR
   $ venv\\Scripts\\activate            # Windows

2. Install dependencies:
   $ pip install -r requirements.txt

3. Copy and edit configuration:
   $ cp .env.example .env
   # Edit .env with your settings

4. Initialize Kafka topics:
   $ python setup_kafka.py

5. Run the service:
   $ python main.py

6. In another terminal, consume messages:
   $ python consumer_example.py trades 10


OPTION 3: Production Kubernetes Deployment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

(Coming in next phase - manifests for ConfigMaps, Services, Deployments)


================================================================================
ENVIRONMENT CONFIGURATION
================================================================================

Key variables in .env:

# Binance symbols to monitor (comma-separated)
BINANCE_SYMBOLS=BTCUSDT,ETHUSDT,BNBUSDT,ADAUSDT,XRPUSDT

# WebSocket stream types
BINANCE_STREAM_TYPES=aggTrade,depth@100ms,trade,kline_1m

# Kafka broker addresses
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# Kafka topics for different event types
KAFKA_TOPIC_RAW_EVENTS=binance-raw-events
KAFKA_TOPIC_TRADES=binance-trades
KAFKA_TOPIC_DEPTH=binance-depth

# Connection resilience
RECONNECT_INTERVAL=5          # Base reconnect delay (seconds)
MAX_RETRIES=5                 # Max reconnection attempts
BATCH_SIZE=100                # Messages per batch
BATCH_TIMEOUT_SECONDS=5       # Batch flush timeout

# Logging
LOG_LEVEL=INFO                # DEBUG, INFO, WARNING, ERROR
LOG_FORMAT=json               # json or text


================================================================================
TESTING
================================================================================

Run unit tests:
$ pytest tests/ -v

Run with coverage:
$ pytest tests/ --cov=src --cov=config

Individual test files:
$ pytest tests/test_ingestion.py::TestBinanceWebSocketClient -v


================================================================================
MONITORING & DEBUGGING
================================================================================

View service metrics (logged every 30 seconds):
$ docker-compose logs binance-ingestion | grep service_metrics

Example metric output:
{
  "level": "info",
  "event": "service_metrics",
  "messages_received": 45000,
  "messages_processed": 45000,
  "messages_buffered": 5,
  "connected": true,
  "reconnect_count": 0
}

Inspect Kafka topics:
$ docker exec kafka kakfa-topics --list --bootstrap-server localhost:29092

Check topic configuration:
$ docker exec kafka kafka-topics --describe \\
    --topic binance-trades \\
    --bootstrap-server localhost:29092


================================================================================
ARCHITECTURE FLOW
================================================================================

                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Binance API    â”‚
                  â”‚   WebSocket      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    Raw market events
                    (JSON bytes)
                           â”‚
                           â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  BinanceWebSocketClient             â”‚
         â”‚  - Multi-stream subscription        â”‚
         â”‚  - Auto reconnect                   â”‚
         â”‚  - Event enrichment                 â”‚
         â”‚  - Batching (100 msgs / 5 sec)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                  Enriched JSON events
                  (with timestamps)
                           â”‚
                           â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  MarketEventProducer                â”‚
         â”‚  - Kafka serialization              â”‚
         â”‚  - Compression (snappy)             â”‚
         â”‚  - Topic routing                    â”‚
         â”‚  - Error handling & retry           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚          â”‚          â”‚
                â–¼          â–¼          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ RAW      â”‚ â”‚ TRADES   â”‚ â”‚ DEPTH    â”‚
        â”‚ EVENTS   â”‚ â”‚ TOPIC    â”‚ â”‚ TOPIC    â”‚
        â”‚ TOPIC    â”‚ â”‚          â”‚ â”‚          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Downstream (ready for Phase 2):
- PySpark Streaming for real-time aggregations
- Delta Lake for time-travel queries
- Grafana for dashboards


================================================================================
TROUBLESHOOTING
================================================================================

Issue: WebSocket connection refused
â”œâ”€ Check Binance is accessible: curl https://stream.binance.com
â”œâ”€ Verify network/firewall settings
â””â”€ Check logs for detailed error

Issue: Kafka broker not found
â”œâ”€ Ensure Kafka is running: docker-compose up kafka
â”œâ”€ Check KAFKA_BOOTSTRAP_SERVERS in .env
â”œâ”€ Verify Docker network connectivity

Issue: Topics not created
â”œâ”€ Run manually: python setup_kafka.py
â”œâ”€ Check Kafka broker status
â”œâ”€ Verify topic creation permissions

Issue: Memory usage too high
â”œâ”€ Reduce BATCH_SIZE to 50 or 25
â”œâ”€ Increase BATCH_TIMEOUT_SECONDS to 10
â”œâ”€ Reduce number of BINANCE_SYMBOLS

Issue: Message processing is slow
â”œâ”€ Check Kafka broker capacity
â”œâ”€ Verify network bandwidth
â”œâ”€ Increase BATCH_SIZE to 200
â”œâ”€ Scale to multiple consumer instances


================================================================================
SUCCESS INDICATORS
================================================================================

âœ… Service started successfully:
   - "ingestion_service_initialized" log message
   - No connection errors in logs

âœ… WebSocket connected:
   - "websocket_connected" log message
   - Connection status shows "true" in metrics

âœ… Data flowing:
   - "service_metrics" logged every 30 seconds
   - messages_received > 0
   - messages_processed > 0

âœ… Kafka working:
   - Messages visible in Kafka UI (http://localhost:8080)
   - Console consumer shows events
   - Topic metrics show message counts


================================================================================
NEXT STEPS
================================================================================

1. Monitor the ingestion service
   - Check metrics in logs every 30 seconds
   - Access Kafka UI to see message flow

2. Verify Kafka topics are receiving data
   - Use consumer_example.py to inspect messages
   - Sample events from different topic types

3. Scale the configuration
   - Add more symbols: LTCUSDT, DOTUSDT, etc.
   - Add more stream types: spot@balance, listenKey updates
   - Increase partitions for parallel processing

4. Prepare for Phase 2: PySpark Processing
   - Design aggregation windows (1m, 5m, 1h)
   - Plan feature engineering (volume spikes, volatility)
   - Set up Delta Lake storage

5. Monitor in production
   - Set up alerting on connection failures
   - Track message lag (Kafka lag monitoring)
   - Monitor resource usage (CPU, memory, network)

"""

GETTING_STARTED = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     BINANCE REAL-TIME INGESTION - GETTING STARTED IN 5 MINUTES               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FASTEST START (Docker Compose):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Start all services:
   
   $ cd ingestion
   $ docker-compose up -d
   
   âœ“ Zookeeper started on port 2181
   âœ“ Kafka started on port 9092
   âœ“ Kafka UI running on http://localhost:8080
   âœ“ Binance ingestion service started and connecting
   

2. Wait 10 seconds for connection, then check status:
   
   $ docker-compose logs binance-ingestion
   
   Look for:
   "ingestion_service_initialized"
   "websocket_connected"
   

3. View messages in Kafka UI:
   
   Open http://localhost:8080 in browser
   â†“
   Click "Topics" in left menu
   â†“
   Select "binance-trades" topic
   â†“
   Scroll to Messages section
   â†“
   Click "Fetch messages" - you should see real-time trades!
   

4. Test with consumer script:
   
   $ python consumer_example.py trades 10
   
   Example output:
   [1] BTCUSDT: 0.5 @ 45000.00
   [2] ETHUSDT: 2.0 @ 2500.00
   [3] BNBUSDT: 10.0 @ 300.00
   ...
   

5. Monitor metrics (every 30 seconds):
   
   $ docker-compose logs -f binance-ingestion | grep service_metrics
   
   Example output:
   {
     "messages_received": 45000,
     "messages_processed": 45000,
     "connected": true
   }
   

6. Stop when done:
   
   $ docker-compose down
   
   All containers shut down gracefully


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHAT'S HAPPENING BEHIND THE SCENES:

  Binance WebSocket (50K+ msg/sec)
           â†“ [parsed, enriched]
  Python WebSocket Client
           â†“ [batched: 100 msgs or 5 sec]
  In-Memory Buffer
           â†“
  Kafka Producer
           â†“
  Kafka Broker (3 topics with compression)
           â†“
  Ready for downstream processing (PySpark, etc.)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KEY FILES TO UNDERSTAND:

  src/binance_client.py (444 lines) - Main WebSocket client
    â”œâ”€ Connects to Binance streams
    â”œâ”€ Handles reconnections automatically
    â”œâ”€ Buffers and batches messages
    â””â”€ Sends to Kafka producer

  src/kafka_producer.py (233 lines) - Kafka producer
    â”œâ”€ Serializes events to JSON
    â”œâ”€ Routes to appropriate topics
    â”œâ”€ Handles errors and retries
    â””â”€ Tracks metrics

  main.py - Entry point
    â”œâ”€ Initializes configuration
    â”œâ”€ Starts WebSocket client
    â”œâ”€ Logs metrics every 30 seconds
    â””â”€ Handles shutdown signals


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CUSTOMIZATION:

Edit .env file to:

1. Add more symbols:
   BINANCE_SYMBOLS=BTCUSDT,ETHUSDT,BNBUSDT,ADAUSDT,XRPUSDT,LTCUSDT,DOTUSDT

2. Change stream types:
   BINANCE_STREAM_TYPES=aggTrade,depth@100ms

3. Adjust batching:
   BATCH_SIZE=200
   BATCH_TIMEOUT_SECONDS=10

4. Change log verbosity:
   LOG_LEVEL=DEBUG


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NEXT: EXPLORE THE ARCHITECTURE

  Read: ARCHITECTURE.md - for complete system design
  Read: README.md - for full documentation
  
  You're now ready to build Phase 2: PySpark Stream Processing! ğŸš€


"""

if __name__ == "__main__":
    print(PROJECT_STRUCTURE)
    print("\n\n")
    print(GETTING_STARTED)
